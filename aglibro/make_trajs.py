import os
from aglibro.util.utils import load_jsonl
import argparse
from pathlib import Path
import json

def parse_log(log_file):
    try:
        with open(log_file, 'r') as f:
            lines = f.readlines()
            deny_lines_id = set()
            for i, line in enumerate(lines):
                if ', skipping.' in line and 'already exists in' in line:
                    deny_lines_id.add(i)
                    deny_lines_id.add(i - 1)
            
            log = "".join([line for i, line in enumerate(lines) if i not in deny_lines_id])
            return log
    except FileNotFoundError:
        return ""

def location_log(instance_id: str, location_dir: str):
    logs_dir = Path(location_dir) / 'localization_logs'
    log_file = logs_dir / f'{instance_id}.log'
    return parse_log(log_file)
    
def location_merged_log(instance_id: str, location_merged_dir: str):
    logs_dir = Path(location_merged_dir) / 'localization_logs'
    log_file = logs_dir / f'{instance_id}.log'
    return parse_log(log_file)

def repair_run_log(instance_id: str, repair_run_dir: str):
    logs_dir = Path(repair_run_dir) / 'localization_logs'
    log_file = logs_dir / f'{instance_id}.log'
    return parse_log(log_file)

def rerank_log(instance_id: str, rerank_dir: str):
    logs_dir = Path(rerank_dir) / 'instance_logs'
    log_file = logs_dir / f'{instance_id}' / f'{instance_id}.log'
    return parse_log(log_file)

def libro_log(instance_id: str, libro_dir: str):
    logs_dir = Path(libro_dir) / 'instance_logs'
    log_file = logs_dir / f'{instance_id}' / f'{instance_id}.log'
    return parse_log(log_file)

def feedback_log(instance_id: str, feedback_dir: str):
    logs_dir = Path(feedback_dir) / 'instance_logs'
    log_file = logs_dir / f'{instance_id}' / f'{instance_id}.log'
    return parse_log(log_file)

def final_rank_log(instance_id: str, final_rank_dir: str):
    logs_dir = Path(final_rank_dir) / 'instance_logs'
    log_file = logs_dir / f'{instance_id}' / f'{instance_id}.log'
    return parse_log(log_file)

def load_jsonl_and_make_map(json_file):
    jsonl = load_jsonl(json_file)
    return {item['instance_id']: item for item in jsonl}

def load_shared_results_jsons(results_dir, args):
    global location_jsons, location_merged_jsons, repair_run_output_jsons, repair_run_output_norm_jsons, rerank_jsons, libro_jsons
    location_jsons = load_jsonl_and_make_map(results_dir / args.location_dir / 'loc_outputs.jsonl')
    location_merged_jsons = [load_jsonl_and_make_map(results_dir / args.location_merged_dir / 'loc_merged_0-1_outputs.jsonl'), load_jsonl_and_make_map(results_dir / args.location_merged_dir / 'loc_merged_2-3_outputs.jsonl')]
    
    repair_run_output_jsons = []
    repair_run_output_norm_jsons = []
    for repair_run_dir in args.repair_run_dirs:
        repair_run_output_jsons.append(load_jsonl_and_make_map(results_dir / repair_run_dir / 'output.jsonl'))
        norm_jsons = []
        for i in range(21):
            norm_jsons.append(load_jsonl_and_make_map(results_dir / repair_run_dir / f'output_{i}_normalized.jsonl'))
        repair_run_output_norm_jsons.append(norm_jsons)
        
    rerank_jsons = load_jsonl_and_make_map(results_dir / args.rerank_dir / 'success_patches.jsonl')
    libro_jsons = load_jsonl_and_make_map(results_dir / args.libro_dir / 'final_tests.jsonl')
    
def location_json(instance_id):
    return location_jsons[instance_id]
def location_merged_json(instance_id, merged_id):
    return location_merged_jsons[merged_id][instance_id]
def repair_run_output_json(instance_id, repair_run_id):
    repair_jsons = []
    if instance_id not in repair_run_output_jsons[repair_run_id]:
        return []
    for i in range(21):
        repair_json = { "patch_id": i }
        repair_json.update(repair_run_output_norm_jsons[repair_run_id][i][instance_id])
        del repair_json["prompt"]
        
        repair_json['traj'] = repair_run_output_jsons[repair_run_id][instance_id]['traj'][i]
        # print(repair_run_output_jsons[repair_run_id][instance_id]['file_names'])
        repair_json['original_file_name'] = repair_run_output_jsons[repair_run_id][instance_id]['file_names'][0][i]
        repair_jsons.append(repair_json)
    return repair_jsons
def rerank_json(instance_id):
    return rerank_jsons[instance_id]
def libro_json(instance_id, libro_dir):
    final_json_file = libro_dir / 'instance_logs' / f'{instance_id}' / f'{instance_id}_final_tests.json'
    try:
        with open(final_json_file, 'r') as f:
            final_json = json.load(f)
    except FileNotFoundError:
        return {
            "instance_id": instance_id,
            "tests": [],
            "final_tests": [],
            "usage": [],
            "hints": "For some unknown reason, no tests were generated by LIBRO."
        }
    libro_res = {
        "instance_id": instance_id,
        "tests": [],
        "final_tests": final_json["final_tests"],
        "usage": final_json["usage"],
    }
    for i in range(10):
        test_json_file = libro_dir / 'instance_logs' / f'{instance_id}' / f'{instance_id}_test_{i}.json'
        with open(test_json_file, 'r') as f:
            test_json = json.load(f)
        libro_res["tests"].append(test_json)
    return libro_res
def feedback_json(instance_id, feedback_dir, all_preds):
    instance_dir = feedback_dir / 'instance_logs' / f'{instance_id}'
    # 统计目录下有多少格式形如 {instance_id}_{i}.json 的文件，其中 i 是数字
    json_files = list(instance_dir.glob(f'{instance_id}_*.json'))
    file_count = len(json_files)
    try:
        used_tests = libro_jsons[instance_id]["final_tests"]
    except KeyError:
        used_tests = []
    used_tests.sort(key=lambda x: len(x))
    used_tests = used_tests[:3]
    res = {
        "instance_id": instance_id,
        "libro_tests_num": len(used_tests),
        "patches_num": min(20, len(rerank_jsons[instance_id]["edits"])),
        "used_tests": used_tests,
        "feedback": [],
        # "pred": all_preds[instance_id]
    }
    assert file_count == res['patches_num'] * res['libro_tests_num']
    for i in range(res['patches_num']):
        res['feedback'].append({
            "patch_id": i,
            "original_patch": rerank_jsons[instance_id]["edits"][i]['model_patch'],
            "feedback_per_test": []
        })
        for j in range(res['libro_tests_num']):
            feedback_file = instance_dir / f'{instance_id}_{i * len(used_tests) + j}.json'
            with open(feedback_file, 'r') as f:
                feedback_json = json.load(f)
            feedback_json['test_id'] = j
            feedback_json['libro_test'] = used_tests[j]
            res['feedback'][i]['feedback_per_test'].append(feedback_json)
    return res
def final_rank_json(instance_id, final_rank_dir):
    final_rank_json_file = final_rank_dir / 'instance_logs' / f'{instance_id}' / f'{instance_id}.json'
    with open(final_rank_json_file, 'r') as f:
        final_rank_json = json.load(f)
    return final_rank_json

parser = argparse.ArgumentParser()
parser.add_argument('--results_dir', type=str, default='results/aglibro-feedback-1105')
parser.add_argument('--location_dir', type=str, default='location')
parser.add_argument('--location_merged_dir', type=str, default='location_merged')
parser.add_argument('--repair_run_dirs', nargs="+", type=str, help="List of repair run directories", default=['repair_run_1', 'repair_run_2'])
parser.add_argument('--rerank_dir', type=str, default='rerank')
parser.add_argument('--libro_dir', type=str, default='libro')
parser.add_argument('--feedback_dir', type=str, default='feedback')
parser.add_argument('--final_rank_dir', type=str, default='restore-rank')
parser.add_argument('--output_dir', type=str, default='trajs')
parser.add_argument('--traj_type', type=str, choices=['logs', 'jsons'], default='logs')

args = parser.parse_args()

results_dir = Path(args.results_dir)
output_folder = results_dir / args.output_dir
if not output_folder.exists():
    output_folder.mkdir()

# 先读取最后成功生成的 cases
preds_file = results_dir / args.feedback_dir / 'all_preds.jsonl'
all_preds = load_jsonl(preds_file)
instance_ids = [pred['instance_id'] for pred in all_preds]
all_preds = {pred['instance_id']: pred for pred in all_preds}

if args.traj_type == 'logs':
    for instance_id in instance_ids:
        traj = ""
        traj += "======================================== Location ========================================\n"
        traj += location_log(instance_id, results_dir / args.location_dir)
        traj += "\n==================================== End of Location =====================================\n\n"
        
        # traj += "======================================== Location Merged ========================================\n"
        # traj += location_merged_log(instance_id, results_dir / args.location_merged_dir)
        # traj += "\n==================================== End of Location Merged =====================================\n\n"
        # location_merged 好像不会生成 log
        
        for repair_run_dir in args.repair_run_dirs:
            traj += f"======================================== {repair_run_dir} ========================================\n"
            traj += repair_run_log(instance_id, results_dir / repair_run_dir)
            traj += f"\n==================================== End of {repair_run_dir} =====================================\n\n"
            
        traj += "======================================== Rerank ========================================\n"
        traj += rerank_log(instance_id, results_dir / args.rerank_dir)
        traj += "\n==================================== End of Rerank =====================================\n\n"
        
        traj += "======================================== Libro ========================================\n"
        traj += libro_log(instance_id, results_dir / args.libro_dir)
        traj += "\n==================================== End of Libro =====================================\n\n"
        
        traj += "======================================== Feedback ========================================\n"
        traj += feedback_log(instance_id, results_dir / args.feedback_dir)
        traj += "\n==================================== End of Feedback =====================================\n\n"
        
        traj += "======================================== Final Rank ========================================\n"
        traj += final_rank_log(instance_id, results_dir / args.final_rank_dir)
        traj += "\n==================================== End of Final Rank =====================================\n\n"
        
        with open(output_folder / f'{instance_id}.log', 'w') as f:
            f.write(traj)
else:
    load_shared_results_jsons(results_dir, args)
    for instance_id in instance_ids:
        traj = {
            "instance_id": instance_id,
            "location": location_json(instance_id),
            # "location_merged": [location_merged_json(instance_id, 0), location_merged_json(instance_id, 1)],
            # "repair_runs": {
            #     "repair_run_1": repair_run_output_json(instance_id, 0),
            #     "repair_run_2": repair_run_output_json(instance_id, 1)
            # }
            "location_merged_and_repair_runs": [
                {
                    "location_id": i,
                    "location_merged": location_merged_json(instance_id, i),
                    "repair_runs": repair_run_output_json(instance_id, i)
                } for i in range(2)
            ],
            "rerank": rerank_json(instance_id),
            "libro": libro_json(instance_id, results_dir / args.libro_dir),
            "feedback": feedback_json(instance_id, results_dir / args.feedback_dir, all_preds),
            "final_rank": final_rank_json(instance_id, results_dir / args.final_rank_dir),
            "pred": all_preds[instance_id]
        }
        with open(output_folder / f'{instance_id}.json', 'w') as f:
            json.dump(traj, f, indent=2)
import argparse
import json
import os

from collections import Counter, OrderedDict
from pathlib import Path

from tqdm import tqdm

from aglibro.util.postprocess_data import extract_python_blocks, normalize_patch
from aglibro.util.utils import load_json, load_jsonl

from aglibro.feedback.test import run_test

import argparse
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import os
from pathlib import Path
import re
import traceback
from difflib import unified_diff
import docker
import logging
import copy

from datasets import load_dataset
from tqdm import tqdm
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from docker.models.containers import Container

from aglibro.util.utils import (
    load_existing_instance_ids,
    load_json,
    load_jsonl,
    # setup_logger,
)
from aglibro.util.model import (
    make_model,
    get_model_price
)
from aglibro.util.api_requests import num_tokens_from_messages
from aglibro.util.postprocess_data import (
    check_code_differ_by_just_empty_lines,
    check_syntax,
    extract_python_blocks,
    fake_git_repo,
    lint_code,
    parse_diff_edit_commands,
    parse_edit_commands,
    remove_empty_lines,
    split_edit_multifile_commands,
)
from aglibro.util.preprocess_data import (
    get_full_file_paths_and_classes_and_functions,
    get_repo_structure,
    line_wrap_content,
    transfer_arb_locs_to_locs,
)
from aglibro.util.postprocess_tests import (
    # make_test_script,
    parse_output,
    get_logs_eval,
    get_logs_eval_with_repo,
    MAP_REPO_TO_TEST_PATH,
)
from aglibro.docker.docker_utils import (
    remove_image,
    copy_to_container,
    exec_run_with_timeout,
    cleanup_container,
    list_images,
    should_remove,
    clean_images,
)
from aglibro.docker.docker_build import (
    BuildImageError,
    build_container,
    build_env_images,
    close_logger,
    setup_logger,
    INSTANCE_IMAGE_BUILD_DIR,
)
from swebench.harness.constants import (
    MAP_REPO_VERSION_TO_SPECS,
    APPLY_PATCH_FAIL,
    APPLY_PATCH_PASS,
    KEY_INSTANCE_ID,
    RUN_EVALUATION_LOG_DIR,
)
from swebench.harness.grading import get_eval_report
from swebench.harness.test_spec import make_test_spec, TestSpec
from swebench.harness.utils import load_swebench_dataset, str2bool
from swebench.harness.run_evaluation import EvaluationError, get_dataset_from_preds

from aglibro.libro.llm_prompt import generate_tests
from aglibro.libro.postprocess import run_generate_test
from aglibro.libro.llm_regenerate import regenerate_tests

from aglibro.feedback.test import make_test_script, run_test
from aglibro.repair.repair import construct_topn_file_context, _post_process_multifile_repair, post_process_raw_output
from aglibro.util.postprocess_data import extract_python_blocks, normalize_patch

from aglibro.feedback.feedback import trans_edit_to_patch

LLM_PROMPT = """
We are currently solving the following issue within our repository. Here is the issue text:
--- BEGIN ISSUE ---
{problem_statement}
--- END ISSUE ---

{file_prompt}

Now we have generated some patches to fix this bug. Each patch is represented as *SEARCH/REPLACE* pairs.

Every *SEARCH/REPLACE* edit uses this format:
1. The file path
2. The start of search block: <<<<<<< SEARCH
3. A contiguous chunk of lines to search for in the existing source code
4. The dividing line: =======
5. The lines to replace into the source code
6. The end of the replace block: >>>>>>> REPLACE

Here is an example of *SEARCH/REPLACE* pairs:

```python
### mathweb/flask/app.py
<<<<<<< SEARCH
from flask import Flask
=======
import math
from flask import Flask
>>>>>>> REPLACE
```

Below are some patches generated, which are expected to fix the bug.
We can tell you that this edits are generated by a model, and we initially generated 42 patches. After the normalization process, we run a majority vote to select the top 3 patches. So we will provide the number of repeats for each patch, and you can use this information to help you make a decision.
--- BEGIN EDITS ---
{edits}
--- END EDITS ---

{order}
"""

ORDER_PROMPT_SELECT = """
Please select the patch you think is the most likely to fix the bug.
Your answer should contain a brief explanation, and the last line should contain the letter corresponding to the patch you selected (A, B, C), without any additional characters or spaces.
"""

ORDER_PROMPT_NEW = """
Please generate a new *SEARCH/REPLACE* edit that you think is the most likely to fix the bug, with the help of the three patches provided above.
The *SEARCH/REPLACE* pairs should be in the same format as the patches provided above, and remember to wrap the *SEARCH/REPLACE* edit in blocks ```python...```.
"""

# 让 LLM 针对唯一的这个 patch 进行优化，关注能否解决错误，以及语法问题，并生成一个新的 patch。如果 LLM 认为没有问题，可以保持 patch 不变。
ORDER_PROMPT_UPDATE = """
The patch we provided above is the one we think is the most likely to fix the bug.
You should check if the patch can solve the bug and if there are any syntax issues, then please optimize the patch to generate a new *SEARCH/REPLACE* edit that you think is more likely to fix the bug.
If you think the patch is correct, you can keep the patch unchanged, but you should also print such a *SEARCH/REPLACE* pair in your answer.
The *SEARCH/REPLACE* pair should be in the same format as the patches provided above, and remember to wrap the *SEARCH/REPLACE* edit in blocks ```python...```.
"""

FILE_PROMPT_ONE = """
Below are some code segments, each from a relevant file. One or more of these files may contain bugs.
In the file below, "..." refers to some less relevant content being omited for brebity.
--- BEGIN FILE ---
{file_content}
--- END FILE ---
"""

FILE_PROMPT_TWO = """
Below are some code segments, each from a relevant file. One or more of these files may contain bugs.
We obtained two groups of file segments, and we will present them separately below. The contents of two groups may overlap.
In the file below, "..." refers to some less relevant content being omited for brebity.
--- BEGIN FILE 1 ---
{file_1}
--- END FILE 1 ---

--- BEGIN FILE 2 ---
{file_2}
--- END FILE 2 ---
"""

EDIT_IN_PROMPT = """
--- BEGIN EDIT {i} ---
```python
{edit}
```
This edit repeated {repeat} time(s).
--- END EDIT {i} ---
"""

def make_prompt(
    problem_statement: str,
    patches: list[dict],
    type: str
) -> str:
    file_contens = set()
    edit_prompts = []
    for i, patch in enumerate(patches):
        if type == "update_top1" and i:
            break
        patch_prompt = patch['prompt']
        file_content = patch_prompt.split('--- BEGIN FILE ---\n')[1].split('\n--- END FILE ---')[0]
        file_contens.add(file_content)
        
        edit_prompts.append(EDIT_IN_PROMPT.format(
            i=['A', 'B', 'C'][i],
            edit=patch['raw_edit'],
            repeat=patch['repeat']
        ))
    
    file_contens = list(file_contens)
    if len(file_contens) == 1:
        file_prompt = FILE_PROMPT_ONE.format(
            file_content=file_contens[0]
        )
    else:
        file_prompt = FILE_PROMPT_TWO.format(
            file_1=file_contens[0],
            file_2=file_contens[1],
        )
    
    edit_prompt = "\n\n".join(edit_prompts)
    if type == "update_top1":
        order = ORDER_PROMPT_UPDATE
    elif type == "select":
        order = ORDER_PROMPT_SELECT
    else:
        order = ORDER_PROMPT_NEW
    
    prompt = LLM_PROMPT.format(
        problem_statement=problem_statement,
        file_prompt=file_prompt,
        edits=edit_prompt,
        order=order
    )
    
    messages = [{
        "role": "user",
        "content": prompt
    }]
    
    return messages
    

def ask_llm_to_select(
    instance: dict,
    model,
    temperature: float,
    logger,
    patches: list[dict],
    type: str
):
    instance_id = instance["instance_id"]
    repo = instance["repo"]
    version = instance["version"]
    problem_statement = instance["problem_statement"]
    
    prompt = make_prompt(problem_statement, patches, type)

    logger.info(f"Prompt: {prompt[0]['content']}")
    
    maked_model = make_model(
        model = model,
        logger = logger,
        backend = 'openai' if model.startswith('gpt') else 'deepseek',
        temperature = temperature,
        max_tokens = 1024,
        batch_size = 1
    )
    trajs = maked_model.codegen(prompt, num_samples=1)
    assert len(trajs) == 1, f"Expected 1 trajectory, got {len(trajs)}"
    
    traj = trajs[0]
    traj['prompt'] = prompt[0]['content']
    usage = traj["usage"]
    cost_per_input_token, cost_per_output_token = get_model_price(model)
    usage['cost'] = cost_per_input_token * usage['prompt_tokens'] + cost_per_output_token * usage['completion_tokens']
    
    query_result = traj["response"].strip()
    
    if type == "select":
        choice = query_result.split('\n')[-1].strip()
        if choice in ['A', 'B', 'C'] and ord(choice) - ord('A') < len(patches):
            choice = ord(choice) - ord('A')
        else:
            choice = 0
        return choice, traj
    else:
        if '```python\n' in query_result:
            query_result = query_result.split('```python\n')[1].split('```')[0]
            success = True
        else:
            query_result = ""
            success = False
            
        traj['result'] = query_result
        traj['success'] = success
        
        return query_result, traj

def select_instance(
    instance: dict,
    output_folder: str,
    output_file: str,
    patches: list[dict],
    loc: dict,
    top_n: int,
    type: str,
    model: str,
    temperature: float,
    skip_existing: bool,
    existing_instance_ids: set,
    args: argparse.Namespace
):
    instance_id = instance["instance_id"]
    repo = instance["repo"]
    version = instance["version"]
    
    instance_dir = Path(output_folder) / "instance_logs" / str(instance_id)
    instance["instance_dir"] = instance_dir
    log_file = instance_dir / f"{instance_id}.log"
    logger = setup_logger(instance_id, log_file, mode="a")
    logger.info(f"Processing instance {instance_id}")
    
    if skip_existing and existing_instance_ids and instance_id in existing_instance_ids:
        logger.info(f"Instance {instance_id} already exists in {output_file}, skipping.")
        return

    patches = patches[:top_n]
    if not patches:
        logger.info(f"No patches found for instance {instance_id}, skipping.")
        return
    
    if type == "select":
        choice, traj = ask_llm_to_select(instance, model, temperature, logger, patches, type)
        final_patch = patches[choice]['model_patch']
        
        report_file = instance_dir / f"{instance_id}.json"
        report_file.write_text(json.dumps({
            "instance_id": instance_id,
            "patches": patches,
            "choice": choice,
            "final_patch": final_patch,
            "reason": traj["response"],
            "traj": traj,
            "usage": traj["usage"]
        }, indent=4))
    else:
        pred_files = loc["found_files"][: args.top_n]
        structure = get_repo_structure(
            instance_id, instance["repo"], instance["base_commit"], "playground"
        )
        files, _, _ = get_full_file_paths_and_classes_and_functions(structure)
        # Construct file contents
        file_contents = dict()
        for i, pred_file in enumerate(pred_files):
            content = None

            for file_content in files:
                if file_content[0] == pred_file:
                    content = "\n".join(file_content[1])
                    file_contents[pred_file] = content
                    break
            assert content is not None, f"{pred_file} file not found"
                
        # Construct top-n file context
        file_to_edit_locs = dict()
        for i, pred_file in enumerate(pred_files):
            if "found_edit_locs" in loc and len(loc["found_edit_locs"]) > i:
                file_to_edit_locs[pred_file] = loc["found_edit_locs"][i]

        topn_content, file_loc_intervals = construct_topn_file_context(
            file_to_edit_locs,
            pred_files,
            file_contents,
            structure,
            context_window=args.context_window,
            loc_interval=args.loc_interval,
            fine_grain_loc_only=args.fine_grain_loc_only,
            add_space=args.add_space,
            no_line_number=args.diff_format,
            sticky_scroll=args.sticky_scroll,
        )
        
        edit, traj = ask_llm_to_select(instance, model, temperature, logger, patches, type)
        trans_res = trans_edit_to_patch(traj['response'], logger, traj, loc, file_contents, file_loc_intervals, args)
        final_patch = trans_res['model_patch']
        if final_patch == "" or final_patch is None:
            logger.info(f"Instance {instance_id} failed to generate a patch.")
            logger.info(f"Use the original patch.")
            final_patch = patches[0]['model_patch']
            
        report_file = instance_dir / f"{instance_id}.json"
        report_file.write_text(json.dumps({
            "instance_id": instance_id,
            "patches": patches,
            "final_patch": final_patch,
            "edit": edit,
            "traj": traj,
            "usage": traj["usage"]
        }, indent=4))
        
    with open(output_file, "a") as f:
        f.write(json.dumps({
            "model_name_or_path": "aglibro",
            "instance_id": instance_id,
            "model_patch": final_patch
        }) + "\n")

def select_tests(
    dataset: str,
    split: str,
    output_folder: str,
    output_file: str,
    patches_file: str,
    loc_file: str,
    top_n: int,
    type: str,
    model: str,
    temperature: float,
    num_threads: int,
    target_ids: list,
    skip_existing: bool,
    args: argparse.Namespace
):
    bench = load_dataset(dataset, split=split)
    existing_instance_ids = load_existing_instance_ids(output_file) if skip_existing else set()
    
    all_patches = load_jsonl(patches_file)
    all_patches = {x["instance_id"]: x["all_patches"] for x in all_patches}
    
    source_ids = set(all_patches.keys())
    target_ids = set(target_ids) if target_ids else source_ids
    if not target_ids.issubset(source_ids):
        print(f"{len(target_ids - source_ids)} instances not found in patches file.")
        target_ids = target_ids & source_ids
    instances = [x for x in bench if x["instance_id"] in target_ids]
    
    if type != "select":
        locs = load_jsonl(loc_file)
        locs = {x["instance_id"]: x for x in locs}
    else:
        locs = {}
    
    # run reproduction in parallel
    print(f"Running {len(target_ids)} instances...")
    with tqdm(total=len(target_ids), smoothing=0) as pbar:
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            # Create a future for running each instance
            futures = {
                executor.submit(
                    select_instance,
                    instance,
                    output_folder,
                    output_file,
                    all_patches[instance['instance_id']],
                    locs.get(instance['instance_id']),
                    top_n,
                    type,
                    model,
                    temperature,
                    skip_existing,
                    existing_instance_ids,
                    args,
                ): None
                for instance in instances
            }
            # Wait for each future to complete
            for future in as_completed(futures):
                pbar.update(1)
                try:
                    # Update progress bar, check if instance ran successfully
                    future.result()
                except Exception as e:
                    traceback.print_exc()
                    continue
    print("All instances run.")

def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--dataset", type=str, default="princeton-nlp/SWE-bench_Lite")
    parser.add_argument("--split", type=str, default="test")
    
    parser.add_argument("--output_folder", type=str, required=True)
    parser.add_argument("--output_file", type=str, default="all_preds.jsonl")
    parser.add_argument("--patches_file", type=str, required=True)
    parser.add_argument("--loc_file", type=str, required=True)
    parser.add_argument("--top_n", type=int, default=3)
    parser.add_argument("--type", type=str, choices=['select', 'generate', 'update_top1'], default='select')
    
    parser.add_argument("--loc_interval", action="store_true")
    parser.add_argument("--context_window", type=int, default=10)
    parser.add_argument("--add_space", action="store_true")
    parser.add_argument("--cot", action="store_true")
    parser.add_argument("--fine_grain_loc_only", action="store_true")
    parser.add_argument("--diff_format", action="store_true")
    parser.add_argument("--skip_greedy", action="store_true")
    parser.add_argument("--sticky_scroll", action="store_true")
    
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o-mini",
        # choices=["gpt-4o", "deepseek-coder", "gpt-4o-mini"],
    )
    parser.add_argument("--temperature", type=float, default=0.0)
    parser.add_argument(
        "--num_threads",
        type=int,
        default=1,
        help="Number of threads to use for creating API requests",
    )
    
    parser.add_argument("--target_ids", nargs="+", type=str, help="Instance IDs to run (space separated)")
    parser.add_argument(
        "--skip_existing",
        action="store_true",
        help="Skip generating of instance id's which already contain a localization in the output file.",
    )
    
    
    args = parser.parse_args()
    args.output_file = os.path.join(args.output_folder, args.output_file)
    
    if not os.path.exists(args.output_folder):
        os.makedirs(args.output_folder)
    
    with open(f"{args.output_folder}/args.json", "w") as f:
        json.dump(vars(args), f, indent=4)
    
    logging.getLogger("httpx").setLevel(logging.CRITICAL)
    
    select_tests(
        dataset = args.dataset,
        split = args.split,
        output_folder = args.output_folder,
        output_file = args.output_file,
        patches_file = args.patches_file,
        loc_file = args.loc_file,
        top_n = args.top_n,
        type = args.type,
        model = args.model,
        temperature = args.temperature,
        num_threads = args.num_threads,
        target_ids = args.target_ids,
        skip_existing = args.skip_existing,
        args = args
    )
    
if __name__ == "__main__":
    main()